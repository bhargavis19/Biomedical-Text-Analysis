{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11680763,"sourceType":"datasetVersion","datasetId":7331133},{"sourceId":11683928,"sourceType":"datasetVersion","datasetId":7333235},{"sourceId":11689269,"sourceType":"datasetVersion","datasetId":7336743}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"/kaggle/input/data-clean/mtsamples_cleaned (2).csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:55:40.405983Z","iopub.execute_input":"2025-05-05T05:55:40.406654Z","iopub.status.idle":"2025-05-05T05:55:40.521490Z","shell.execute_reply.started":"2025-05-05T05:55:40.406619Z","shell.execute_reply":"2025-05-05T05:55:40.520650Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"pip install sumy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T05:55:54.869398Z","iopub.execute_input":"2025-05-05T05:55:54.869690Z","iopub.status.idle":"2025-05-05T05:56:03.220546Z","shell.execute_reply.started":"2025-05-05T05:55:54.869670Z","shell.execute_reply":"2025-05-05T05:56:03.219657Z"}},"outputs":[{"name":"stdout","text":"Collecting sumy\n  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\nCollecting docopt<0.7,>=0.6.1 (from sumy)\n  Downloading docopt-0.6.2.tar.gz (25 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting breadability>=0.1.20 (from sumy)\n  Downloading breadability-0.1.20.tar.gz (32 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\nCollecting pycountry>=18.2.23 (from sumy)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\nRequirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\nRequirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\nDownloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=426fa2ee208ee860db1c730fab2cb3ecbd26a7db58611c8f0877a1fe512d9d59\n  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0fca793d3fe76c9e8945fee64b4e71bddc204942583ce40a88f55e4a1f6c6b7a\n  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\nSuccessfully built breadability docopt\nInstalling collected packages: docopt, pycountry, breadability, sumy\nSuccessfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install transformers torch datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T06:42:31.579117Z","iopub.execute_input":"2025-05-05T06:42:31.579420Z","iopub.status.idle":"2025-05-05T06:43:45.107096Z","shell.execute_reply.started":"2025-05-05T06:42:31.579399Z","shell.execute_reply":"2025-05-05T06:43:45.106200Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"pip install nlpaug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:06:20.157455Z","iopub.execute_input":"2025-05-05T10:06:20.157659Z","iopub.status.idle":"2025-05-05T10:06:26.897175Z","shell.execute_reply.started":"2025-05-05T10:06:20.157642Z","shell.execute_reply":"2025-05-05T10:06:26.896247Z"}},"outputs":[{"name":"stdout","text":"Collecting nlpaug\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.13.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nlpaug\nSuccessfully installed nlpaug-1.1.11\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')  # Download WordNet for synonym replacement\nnltk.download('punkt')    # Tokenizer\nnltk.download('averaged_perceptron_tagger')  # POS Tagger for NLPAug","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:19:21.877277Z","iopub.execute_input":"2025-05-05T10:19:21.877552Z","iopub.status.idle":"2025-05-05T10:19:22.133068Z","shell.execute_reply.started":"2025-05-05T10:19:21.877533Z","shell.execute_reply":"2025-05-05T10:19:22.132465Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import nltk\nnltk.download('all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:22:59.525149Z","iopub.execute_input":"2025-05-05T10:22:59.525418Z","iopub.status.idle":"2025-05-05T10:23:19.407021Z","shell.execute_reply.started":"2025-05-05T10:22:59.525399Z","shell.execute_reply":"2025-05-05T10:23:19.406325Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading collection 'all'\n[nltk_data]    | \n[nltk_data]    | Downloading package abc to /usr/share/nltk_data...\n[nltk_data]    |   Package abc is already up-to-date!\n[nltk_data]    | Downloading package alpino to /usr/share/nltk_data...\n[nltk_data]    |   Package alpino is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n[nltk_data]    | Downloading package basque_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package basque_grammars is already up-to-date!\n[nltk_data]    | Downloading package bcp47 to /usr/share/nltk_data...\n[nltk_data]    | Downloading package biocreative_ppi to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n[nltk_data]    | Downloading package bllip_wsj_no_aux to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n[nltk_data]    | Downloading package book_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package book_grammars is already up-to-date!\n[nltk_data]    | Downloading package brown to /usr/share/nltk_data...\n[nltk_data]    |   Package brown is already up-to-date!\n[nltk_data]    | Downloading package brown_tei to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package brown_tei is already up-to-date!\n[nltk_data]    | Downloading package cess_cat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_cat is already up-to-date!\n[nltk_data]    | Downloading package cess_esp to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cess_esp is already up-to-date!\n[nltk_data]    | Downloading package chat80 to /usr/share/nltk_data...\n[nltk_data]    |   Package chat80 is already up-to-date!\n[nltk_data]    | Downloading package city_database to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package city_database is already up-to-date!\n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package comparative_sentences to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n[nltk_data]    | Downloading package comtrans to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package comtrans is already up-to-date!\n[nltk_data]    | Downloading package conll2000 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2000 is already up-to-date!\n[nltk_data]    | Downloading package conll2002 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2002 is already up-to-date!\n[nltk_data]    | Downloading package conll2007 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package conll2007 is already up-to-date!\n[nltk_data]    | Downloading package crubadan to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package crubadan is already up-to-date!\n[nltk_data]    | Downloading package dependency_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package dependency_treebank is already up-to-date!\n[nltk_data]    | Downloading package dolch to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/dolch.zip.\n[nltk_data]    | Downloading package english_wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n[nltk_data]    | Downloading package europarl_raw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package europarl_raw is already up-to-date!\n[nltk_data]    | Downloading package extended_omw to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package floresta to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package floresta is already up-to-date!\n[nltk_data]    | Downloading package framenet_v15 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n[nltk_data]    | Downloading package framenet_v17 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package ieer to /usr/share/nltk_data...\n[nltk_data]    |   Package ieer is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package indian to /usr/share/nltk_data...\n[nltk_data]    |   Package indian is already up-to-date!\n[nltk_data]    | Downloading package jeita to /usr/share/nltk_data...\n[nltk_data]    |   Package jeita is already up-to-date!\n[nltk_data]    | Downloading package kimmo to /usr/share/nltk_data...\n[nltk_data]    |   Package kimmo is already up-to-date!\n[nltk_data]    | Downloading package knbc to /usr/share/nltk_data...\n[nltk_data]    |   Package knbc is already up-to-date!\n[nltk_data]    | Downloading package large_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package large_grammars is already up-to-date!\n[nltk_data]    | Downloading package lin_thesaurus to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n[nltk_data]    | Downloading package mac_morpho to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mac_morpho is already up-to-date!\n[nltk_data]    | Downloading package machado to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package machado is already up-to-date!\n[nltk_data]    | Downloading package masc_tagged to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package masc_tagged is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping\n[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n[nltk_data]    | Downloading package moses_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package moses_sample is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package mte_teip5 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package mte_teip5 is already up-to-date!\n[nltk_data]    | Downloading package mwa_ppdb to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n[nltk_data]    | Downloading package names to /usr/share/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package nombank.1.0 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package nonbreaking_prefixes to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n[nltk_data]    | Downloading package nps_chat to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package nps_chat is already up-to-date!\n[nltk_data]    | Downloading package omw to /usr/share/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package omw-1.4 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package opinion_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n[nltk_data]    | Downloading package panlex_swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package paradigms to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package paradigms is already up-to-date!\n[nltk_data]    | Downloading package pe08 to /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/pe08.zip.\n[nltk_data]    | Downloading package perluniprops to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping misc/perluniprops.zip.\n[nltk_data]    | Downloading package pil to /usr/share/nltk_data...\n[nltk_data]    |   Package pil is already up-to-date!\n[nltk_data]    | Downloading package pl196x to /usr/share/nltk_data...\n[nltk_data]    |   Package pl196x is already up-to-date!\n[nltk_data]    | Downloading package porter_test to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package porter_test is already up-to-date!\n[nltk_data]    | Downloading package ppattach to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package ppattach is already up-to-date!\n[nltk_data]    | Downloading package problem_reports to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package problem_reports is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_1 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n[nltk_data]    | Downloading package product_reviews_2 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n[nltk_data]    | Downloading package propbank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package propbank is already up-to-date!\n[nltk_data]    | Downloading package pros_cons to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package pros_cons is already up-to-date!\n[nltk_data]    | Downloading package ptb to /usr/share/nltk_data...\n[nltk_data]    |   Package ptb is already up-to-date!\n[nltk_data]    | Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package punkt_tab to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package punkt_tab is already up-to-date!\n[nltk_data]    | Downloading package qc to /usr/share/nltk_data...\n[nltk_data]    |   Package qc is already up-to-date!\n[nltk_data]    | Downloading package reuters to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package reuters is already up-to-date!\n[nltk_data]    | Downloading package rslp to /usr/share/nltk_data...\n[nltk_data]    |   Package rslp is already up-to-date!\n[nltk_data]    | Downloading package rte to /usr/share/nltk_data...\n[nltk_data]    |   Package rte is already up-to-date!\n[nltk_data]    | Downloading package sample_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sample_grammars is already up-to-date!\n[nltk_data]    | Downloading package semcor to /usr/share/nltk_data...\n[nltk_data]    |   Package semcor is already up-to-date!\n[nltk_data]    | Downloading package senseval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package senseval is already up-to-date!\n[nltk_data]    | Downloading package sentence_polarity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentence_polarity is already up-to-date!\n[nltk_data]    | Downloading package sentiwordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sentiwordnet is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package sinica_treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package sinica_treebank is already up-to-date!\n[nltk_data]    | Downloading package smultron to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package smultron is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package spanish_grammars to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package spanish_grammars is already up-to-date!\n[nltk_data]    | Downloading package state_union to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package state_union is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package subjectivity to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package subjectivity is already up-to-date!\n[nltk_data]    | Downloading package swadesh to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package swadesh is already up-to-date!\n[nltk_data]    | Downloading package switchboard to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package switchboard is already up-to-date!\n[nltk_data]    | Downloading package tagsets to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package tagsets is already up-to-date!\n[nltk_data]    | Downloading package tagsets_json to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping help/tagsets_json.zip.\n[nltk_data]    | Downloading package timit to /usr/share/nltk_data...\n[nltk_data]    |   Package timit is already up-to-date!\n[nltk_data]    | Downloading package toolbox to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package toolbox is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package udhr to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr is already up-to-date!\n[nltk_data]    | Downloading package udhr2 to /usr/share/nltk_data...\n[nltk_data]    |   Package udhr2 is already up-to-date!\n[nltk_data]    | Downloading package unicode_samples to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package unicode_samples is already up-to-date!\n[nltk_data]    | Downloading package universal_tagset to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_tagset is already up-to-date!\n[nltk_data]    | Downloading package universal_treebanks_v20 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n[nltk_data]    |       date!\n[nltk_data]    | Downloading package vader_lexicon to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package vader_lexicon is already up-to-date!\n[nltk_data]    | Downloading package verbnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package verbnet is already up-to-date!\n[nltk_data]    | Downloading package verbnet3 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n[nltk_data]    | Downloading package webtext to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package webtext is already up-to-date!\n[nltk_data]    | Downloading package wmt15_eval to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n[nltk_data]    | Downloading package word2vec_sample to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package word2vec_sample is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet2021 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet2022 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n[nltk_data]    | Downloading package wordnet31 to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /usr/share/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to /usr/share/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package ycoe to /usr/share/nltk_data...\n[nltk_data]    |   Package ycoe is already up-to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection all\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport nlpaug.augmenter.word as naw\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport nltk\n\n# Download necessary NLTK resources for POS tagging (required by nlpaug)\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\n\n# Load your existing data\ndf = pd.read_csv(\"/kaggle/input/data-clean/mtsamples_cleaned (2).csv\")\ndf = df.dropna(subset=[\"transcription\"])\n\n# Define nlpaug's Synonym Augmenter\naugmenter = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=5)\n\n# Apply text augmentation using nlpaug\ntqdm.pandas()\ndf['augmented_transcription'] = df['transcription'].progress_apply(lambda x: augmenter.augment(x))\neze().numpy()  # CLS token\n\n# Load ClinicalBERT for embedding extraction\ntokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\nmodel = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n\n# Define function to extract ClinicalBERT embeddings\ndef get_embedding(text):\n    # Tokenize and truncate to the maximum length (512 tokens)\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    return outputs.last_hidden_state[:, 0, :].sque\n# Generate embeddings for original and augmented transcriptions\ndf['embedding_original'] = df['transcription'].progress_apply(get_embedding)\ndf['embedding_augmented'] = df['augmented_transcription'].progress_apply(get_embedding)\n\n# Save embeddings and augmented text to disk\nnp.save('/kaggle/working/embedding_original.npy', np.stack(df['embedding_original'].values))\nnp.save('/kaggle/working/embedding_augmented.npy', np.stack(df['embedding_augmented'].values))\n\n# Save augmented texts to CSV\ndf[['transcription', 'augmented_transcription', 'medical_specialty']].to_csv('/kaggle/working/augmented_texts.csv', index=False)\n\nprint(\"✅ Augmentation and embedding extraction completed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T10:25:40.964550Z","iopub.execute_input":"2025-05-05T10:25:40.965259Z","iopub.status.idle":"2025-05-05T11:27:17.243061Z","shell.execute_reply.started":"2025-05-05T10:25:40.965233Z","shell.execute_reply":"2025-05-05T11:27:17.242356Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n100%|██████████| 4966/4966 [00:59<00:00, 83.35it/s] \n100%|██████████| 4966/4966 [30:09<00:00,  2.74it/s]\n100%|██████████| 4966/4966 [30:25<00:00,  2.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Augmentation and embedding extraction completed.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"/kaggle/input/data-clean/mtsamples_cleaned (2).csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:19.277137Z","iopub.execute_input":"2025-05-05T11:34:19.277629Z","iopub.status.idle":"2025-05-05T11:34:19.392344Z","shell.execute_reply.started":"2025-05-05T11:34:19.277609Z","shell.execute_reply":"2025-05-05T11:34:19.391813Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Encode labels\nle = LabelEncoder()\ndata['label'] = le.fit_transform(data['medical_specialty'])\n\n# Split dataset (80-20) stratified by label\nX_train, X_test, y_train, y_test = train_test_split(\n    data['transcription'], data['label'],\n    test_size=0.2,\n    stratify=data['label'],\n    random_state=42\n)\n\n# Display a few entries\nprint(X_train.head(), y_train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:21.947703Z","iopub.execute_input":"2025-05-05T11:34:21.948273Z","iopub.status.idle":"2025-05-05T11:34:21.968309Z","shell.execute_reply.started":"2025-05-05T11:34:21.948248Z","shell.execute_reply":"2025-05-05T11:34:21.967665Z"}},"outputs":[{"name":"stdout","text":"2255    preoperative diagnosis closed displaced probab...\n1388    chief complaint stage iiic endometrial cancer ...\n4783    endovascular brachytherapy ebtthe patient unde...\n4297    subjective yearold white male established pati...\n3324    chief complaint mental changes todayhistory pr...\nName: transcription, dtype: object 2255    27\n1388    35\n4783     3\n4297     5\n3324    15\nName: label, dtype: int64\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install nlpaug\n!pip install nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:34:42.041958Z","iopub.execute_input":"2025-05-05T11:34:42.042651Z","iopub.status.idle":"2025-05-05T11:34:48.641155Z","shell.execute_reply.started":"2025-05-05T11:34:42.042628Z","shell.execute_reply":"2025-05-05T11:34:48.640205Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\nRequirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (1.26.4)\nRequirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.2.3)\nRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\nRequirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.16.2->nlpaug) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\nRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.13.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.16.2->nlpaug) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.2->nlpaug) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.16.2->nlpaug) (2024.2.0)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.16.2->nlpaug) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:35:05.803467Z","iopub.execute_input":"2025-05-05T11:35:05.803775Z","iopub.status.idle":"2025-05-05T11:35:05.950413Z","shell.execute_reply.started":"2025-05-05T11:35:05.803751Z","shell.execute_reply":"2025-05-05T11:35:05.949557Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='nltk')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:35:21.726125Z","iopub.execute_input":"2025-05-05T11:35:21.726578Z","iopub.status.idle":"2025-05-05T11:35:21.731044Z","shell.execute_reply.started":"2025-05-05T11:35:21.726544Z","shell.execute_reply":"2025-05-05T11:35:21.730316Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nlpaug.augmenter.word as naw\nimport nltk\nfrom collections import Counter\n\n# Assuming you have a DataFrame 'data' with columns 'transcription' (medical transcriptions) and 'medical_specialty' (clinical domain)\n# If not, adapt the code to your actual data structure\n\n# Download necessary NLTK resources for NLP Augmenter\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\n# Display initial class distribution\nprint(\"Original class distribution:\")\nprint(data['medical_specialty'].value_counts())\n\n# 1. NLP Augmenter Method\ndef balance_with_nlp_augmenter(data, target_samples_per_class=1500):\n    \"\"\"\n    Balance dataset using NLP Augmenter by replacing words with synonyms.\n    \n    Args:\n        data: DataFrame with 'transcription' and 'medical_specialty' columns\n        target_samples_per_class: Number of samples to generate for each class\n    \n    Returns:\n        Balanced DataFrame\n    \"\"\"\n    # Initialize synonym augmenter\n    aug = naw.SynonymAug(aug_src='wordnet')\n    \n    balanced_texts = []\n    balanced_categories = []\n    \n    # Process each medical specialty\n    for specialty in data['medical_specialty'].unique():\n        # Get samples of the current specialty\n        specialty_samples = data[data['medical_specialty'] == specialty]\n        \n        # Add original samples\n        balanced_texts.extend(specialty_samples['transcription'].tolist())\n        balanced_categories.extend([specialty] * len(specialty_samples))\n        \n        # Calculate how many additional samples we need\n        n_to_add = max(0, target_samples_per_class - len(specialty_samples))\n        \n        # If we need to add samples\n        if n_to_add > 0:\n            # Augment samples until we reach target\n            samples_to_augment = specialty_samples['transcription'].tolist()\n            augmented_counter = 0\n            \n            while augmented_counter < n_to_add:\n                # Cycle through available samples\n                for text in samples_to_augment:\n                    if augmented_counter >= n_to_add:\n                        break\n                    \n                    # Create augmented text\n                    augmented_text = aug.augment(text)\n                    balanced_texts.append(augmented_text)\n                    balanced_categories.append(specialty)\n                    augmented_counter += 1\n    \n    # Create balanced DataFrame\n    balanced_data = pd.DataFrame({\n        'transcription': balanced_texts,\n        'medical_specialty': balanced_categories\n    })\n    \n    return balanced_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:35:39.288415Z","iopub.execute_input":"2025-05-05T11:35:39.289014Z","iopub.status.idle":"2025-05-05T11:35:39.314297Z","shell.execute_reply.started":"2025-05-05T11:35:39.288989Z","shell.execute_reply":"2025-05-05T11:35:39.313686Z"}},"outputs":[{"name":"stdout","text":"Original class distribution:\nmedical_specialty\nSurgery                          1088\nConsult - History and Phy.        516\nCardiovascular / Pulmonary        371\nOrthopedic                        355\nRadiology                         273\nGeneral Medicine                  259\nGastroenterology                  224\nNeurology                         223\nSOAP / Chart / Progress Notes     166\nUrology                           156\nObstetrics / Gynecology           155\nDischarge Summary                 108\nENT - Otolaryngology               96\nNeurosurgery                       94\nHematology - Oncology              90\nOphthalmology                      83\nNephrology                         81\nEmergency Room Reports             75\nPediatrics - Neonatal              70\nPain Management                    61\nPsychiatry / Psychology            53\nOffice Notes                       50\nPodiatry                           47\nDermatology                        29\nDentistry                          27\nCosmetic / Plastic Surgery         27\nLetters                            23\nPhysical Medicine - Rehab          21\nSleep Medicine                     20\nEndocrinology                      19\nBariatrics                         18\nIME-QME-Work Comp etc.             16\nChiropractic                       14\nDiets and Nutritions               10\nRheumatology                       10\nSpeech - Language                   9\nAutopsy                             8\nLab Medicine - Pathology            8\nAllergy / Immunology                7\nHospice - Palliative Care           6\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Apply the balancing function\nbalanced_data = balance_with_nlp_augmenter(data, target_samples_per_class=1500)\n\n# Display new class distribution\nprint(\"\\nBalanced class distribution:\")\nprint(balanced_data['medical_specialty'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:35:59.804290Z","iopub.execute_input":"2025-05-05T11:35:59.804757Z","iopub.status.idle":"2025-05-05T11:47:35.417173Z","shell.execute_reply.started":"2025-05-05T11:35:59.804733Z","shell.execute_reply":"2025-05-05T11:47:35.416227Z"}},"outputs":[{"name":"stdout","text":"\nBalanced class distribution:\nmedical_specialty\nAllergy / Immunology             1500\nBariatrics                       1500\nCardiovascular / Pulmonary       1500\nNeurology                        1500\nDentistry                        1500\nUrology                          1500\nGeneral Medicine                 1500\nSurgery                          1500\nSpeech - Language                1500\nSOAP / Chart / Progress Notes    1500\nSleep Medicine                   1500\nRheumatology                     1500\nRadiology                        1500\nPsychiatry / Psychology          1500\nPodiatry                         1500\nPhysical Medicine - Rehab        1500\nPediatrics - Neonatal            1500\nPain Management                  1500\nOrthopedic                       1500\nOphthalmology                    1500\nOffice Notes                     1500\nObstetrics / Gynecology          1500\nNeurosurgery                     1500\nNephrology                       1500\nLetters                          1500\nLab Medicine - Pathology         1500\nIME-QME-Work Comp etc.           1500\nHospice - Palliative Care        1500\nHematology - Oncology            1500\nGastroenterology                 1500\nENT - Otolaryngology             1500\nEndocrinology                    1500\nEmergency Room Reports           1500\nDischarge Summary                1500\nDiets and Nutritions             1500\nDermatology                      1500\nCosmetic / Plastic Surgery       1500\nConsult - History and Phy.       1500\nChiropractic                     1500\nAutopsy                          1500\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Ensure all transcriptions are strings, even if they are returned as lists from augmentation\ndef ensure_string(x):\n    if isinstance(x, list):\n        return ' '.join(x)\n    return str(x)\n\n# Apply to the transcription column\nbalanced_data['transcription'] = balanced_data['transcription'].apply(ensure_string)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T11:57:21.977162Z","iopub.execute_input":"2025-05-05T11:57:21.977475Z","iopub.status.idle":"2025-05-05T11:57:22.007224Z","shell.execute_reply.started":"2025-05-05T11:57:21.977455Z","shell.execute_reply":"2025-05-05T11:57:22.006334Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Save the augmented and balanced data to a CSV file\nbalanced_data.to_csv('/kaggle/working/balanced_augmented_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T12:04:15.584137Z","iopub.execute_input":"2025-05-05T12:04:15.584424Z","iopub.status.idle":"2025-05-05T12:04:19.356007Z","shell.execute_reply.started":"2025-05-05T12:04:15.584406Z","shell.execute_reply":"2025-05-05T12:04:19.355444Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"pip install transformers datasets scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T12:05:36.382643Z","iopub.execute_input":"2025-05-05T12:05:36.383293Z","iopub.status.idle":"2025-05-05T12:05:40.568480Z","shell.execute_reply.started":"2025-05-05T12:05:36.383269Z","shell.execute_reply":"2025-05-05T12:05:40.567570Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T12:13:47.335840Z","iopub.execute_input":"2025-05-05T12:13:47.336125Z","iopub.status.idle":"2025-05-05T12:13:59.084110Z","shell.execute_reply.started":"2025-05-05T12:13:47.336107Z","shell.execute_reply":"2025-05-05T12:13:59.083383Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.51.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:01:53.364834Z","iopub.execute_input":"2025-05-05T13:01:53.365135Z","iopub.status.idle":"2025-05-05T13:01:57.001546Z","shell.execute_reply.started":"2025-05-05T13:01:53.365115Z","shell.execute_reply":"2025-05-05T13:01:57.000830Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T12:48:12.320896Z","iopub.execute_input":"2025-05-05T12:48:12.321200Z","iopub.status.idle":"2025-05-05T12:48:12.324934Z","shell.execute_reply.started":"2025-05-05T12:48:12.321178Z","shell.execute_reply":"2025-05-05T12:48:12.324328Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport pickle\n\n# Disable WandB\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Load augmented data\ndata = pd.read_csv('/kaggle/working/balanced_augmented_data.csv')\n\n# Encode labels\nle = LabelEncoder()\ndata['label'] = le.fit_transform(data['medical_specialty'])\n\n# Split data\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    data['transcription'], data['label'], test_size=0.2, stratify=data['label'], random_state=42\n)\n\n# Tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(le.classes_))\n\n# Tokenize function\ndef tokenize_function(example):\n    return tokenizer(example['transcription'], truncation=True, padding='max_length', max_length=512)\n\n# Prepare HuggingFace Datasets\ntrain_dataset = Dataset.from_dict({'transcription': train_texts.tolist(), 'label': train_labels.tolist()})\nval_dataset = Dataset.from_dict({'transcription': val_texts.tolist(), 'label': val_labels.tolist()})\n\n# Tokenize\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n\n# Set format\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# Training arguments WITHOUT evaluation_strategy\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train\nprint(\"🚀 Starting training...\")\ntrainer.train()\nprint(\"✅ Training complete!\")\n\n# Save model and tokenizer\nmodel.save_pretrained('./bert-text-classifier')\ntokenizer.save_pretrained('./bert-text-classifier')\nwith open('./label_encoder.pkl', 'wb') as f:\n    pickle.dump(le, f)\nprint(\"✅ Model and label encoder saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T13:02:25.331085Z","iopub.execute_input":"2025-05-05T13:02:25.331396Z","iopub.status.idle":"2025-05-05T15:23:00.785365Z","shell.execute_reply.started":"2025-05-05T13:02:25.331371Z","shell.execute_reply":"2025-05-05T15:23:00.784188Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/48000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5350fc63053f407185e5a8cb71790104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66c730fae46439eb8eeb6fc325086a1"}},"metadata":{}},{"name":"stdout","text":"🚀 Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9000/9000 2:14:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.752600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.683100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.702100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.742000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.709800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.725600</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>3.683900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.688200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>3.662900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.619400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>3.523600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.457600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>3.361500</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>3.210100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>3.159500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.990000</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.985000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.852100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>2.769700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.559100</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>2.616500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.512000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.372800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.402500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.183200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.230100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.063700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.022800</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.898100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.885700</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.799200</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.623700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.752700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.773500</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.503700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.765100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.506700</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.658000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.572600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.453500</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.445600</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.406300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.359700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.455500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.360200</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.228300</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.357700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.411600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.433800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.307800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.999100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.297000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.222000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.183100</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.166900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.388900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.213200</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.125000</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.306000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.222100</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.119100</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.273200</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.124900</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.983300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.245800</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.240900</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.307300</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.122000</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.170600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.045100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.187500</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.215700</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.101300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.307400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.116100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.206500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.161100</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.137700</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.045400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.180700</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.939200</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.205700</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.962000</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.909700</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.093400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.955800</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.032300</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.996300</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.813200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.075400</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.944900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.870500</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.917600</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.072200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.873900</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.994200</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.987500</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.967300</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.950000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.047800</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.915000</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.884000</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.865500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.901800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.798300</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.976100</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>1.046100</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.941600</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.867700</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.991700</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.751700</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.759700</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.850200</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.949600</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.827600</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>1.013200</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.897600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.963600</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.938000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.887900</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.939300</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>1.050300</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.832600</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.872200</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.947500</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>1.038900</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.929300</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.884500</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>1.125700</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.691800</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.875200</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.813500</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.788900</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>1.048400</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.785900</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.804900</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.902400</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.861400</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.659000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.824700</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.988600</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.862900</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>1.004300</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.775600</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.862200</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.905600</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.729400</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.788900</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.972300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.717000</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.907300</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.839900</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.890800</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.746500</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.769700</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.803000</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.941900</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.823900</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.845000</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.137200</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.793300</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.631700</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.574100</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.922800</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.756800</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.809600</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.750700</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.837300</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.708400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.947000</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.792200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.729000</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.971600</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>1.006800</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.751500</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.811900</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.512500</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.614300</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.751000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.751000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.810500</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.768000</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.668200</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.765300</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.674300</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.764600</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.709800</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.916400</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.831600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.702000</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.630600</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.837800</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.748600</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.720500</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.761600</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.733900</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.742300</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.696300</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.638500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.720200</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.763500</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.573800</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.671700</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.795100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.811000</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.891900</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.659100</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.697300</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.819100</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.616300</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.720800</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.879600</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.728800</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.646700</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.880100</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.797800</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.703100</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.887500</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.747500</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.905800</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.688600</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.635500</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.777600</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.658900</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.737100</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.521600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.740700</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.507100</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.708700</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.923200</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.773000</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.890100</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.812300</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.725700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.645200</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.522500</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.619500</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.831700</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.721800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.764800</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.776000</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.704700</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.748500</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.736900</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.684400</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.626500</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.666000</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.639300</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.781300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.739800</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.775100</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.691000</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.624900</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.738100</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.677900</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.748700</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.754800</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.593200</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.877600</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.527500</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.725600</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.726400</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.603000</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.669900</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.566800</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.615300</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.697900</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.637300</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.632200</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.745600</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.708900</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.519100</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.728600</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.680500</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.713800</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.801700</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.614000</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.661300</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.613900</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.628600</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.757500</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.777300</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.693100</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.628000</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.715500</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.809000</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.823800</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.762300</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.674100</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.803100</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.661200</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.758500</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.568700</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.641700</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.580100</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.547100</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.499200</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.796000</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.548700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.650300</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.573800</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.536300</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.675100</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.582400</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.663600</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.620700</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.606100</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.567200</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.494500</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.660500</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.530500</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.626300</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.614000</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.588200</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.621800</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.559900</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.521000</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.621400</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.736300</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.661300</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.578600</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.792600</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.694800</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.602000</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.620800</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.851300</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.670100</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.810000</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.693700</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.675600</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.719500</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.558800</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.655400</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.621500</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.759500</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.602900</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.656500</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.580200</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.716600</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.486000</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.796600</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.636600</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.566400</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.572100</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.618800</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.690800</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.705300</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.628300</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.587100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.719000</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.674300</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.635400</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.708100</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.651100</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.627000</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.565700</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.715200</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.607700</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.766400</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.591700</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.625200</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.746800</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.567800</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.660200</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.622600</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.696500</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.582700</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.527300</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.587500</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.558200</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.570000</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.546300</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.615500</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.576400</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.629100</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.843300</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.624900</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.598800</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.654700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.659500</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.563000</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.630000</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.617900</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.662700</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.613900</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>0.531700</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>0.524500</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>0.701900</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>0.615600</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.637500</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>0.550100</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>0.788100</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>0.647300</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>0.453400</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.651200</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.576700</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>0.534900</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>0.498000</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>0.907000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.516000</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>0.635900</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>0.561600</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>0.620400</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>0.573100</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.513200</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>0.585600</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>0.751500</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>0.533800</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.695900</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>0.576800</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>0.593300</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>0.611500</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>0.532500</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.575200</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>0.761800</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>0.555600</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>0.618200</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>0.622300</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.582900</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>0.744600</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>0.549900</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>0.601000</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>0.677500</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>0.547200</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>0.594900</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>0.760500</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>0.578600</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>0.541500</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.438400</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>0.670600</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>0.645000</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>0.711400</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>0.571000</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>0.604600</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>0.519800</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>0.736500</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>0.682800</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>0.693800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.691400</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>0.499300</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>0.631900</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>0.612100</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>0.752600</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>0.529200</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>0.673000</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>0.600300</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>0.512000</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>0.528200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.780900</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>0.535900</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>0.513200</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>0.440000</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>0.597900</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>0.623300</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>0.568700</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>0.608200</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>0.593100</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>0.603800</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.737200</td>\n    </tr>\n    <tr>\n      <td>4610</td>\n      <td>0.618100</td>\n    </tr>\n    <tr>\n      <td>4620</td>\n      <td>0.576500</td>\n    </tr>\n    <tr>\n      <td>4630</td>\n      <td>0.597500</td>\n    </tr>\n    <tr>\n      <td>4640</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>0.576900</td>\n    </tr>\n    <tr>\n      <td>4660</td>\n      <td>0.606400</td>\n    </tr>\n    <tr>\n      <td>4670</td>\n      <td>0.616700</td>\n    </tr>\n    <tr>\n      <td>4680</td>\n      <td>0.635800</td>\n    </tr>\n    <tr>\n      <td>4690</td>\n      <td>0.658800</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.573200</td>\n    </tr>\n    <tr>\n      <td>4710</td>\n      <td>0.600100</td>\n    </tr>\n    <tr>\n      <td>4720</td>\n      <td>0.657200</td>\n    </tr>\n    <tr>\n      <td>4730</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>4740</td>\n      <td>0.704100</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>0.607800</td>\n    </tr>\n    <tr>\n      <td>4760</td>\n      <td>0.568400</td>\n    </tr>\n    <tr>\n      <td>4770</td>\n      <td>0.617500</td>\n    </tr>\n    <tr>\n      <td>4780</td>\n      <td>0.629500</td>\n    </tr>\n    <tr>\n      <td>4790</td>\n      <td>0.550400</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.462600</td>\n    </tr>\n    <tr>\n      <td>4810</td>\n      <td>0.752000</td>\n    </tr>\n    <tr>\n      <td>4820</td>\n      <td>0.540000</td>\n    </tr>\n    <tr>\n      <td>4830</td>\n      <td>0.534200</td>\n    </tr>\n    <tr>\n      <td>4840</td>\n      <td>0.546100</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>0.707700</td>\n    </tr>\n    <tr>\n      <td>4860</td>\n      <td>0.543400</td>\n    </tr>\n    <tr>\n      <td>4870</td>\n      <td>0.621100</td>\n    </tr>\n    <tr>\n      <td>4880</td>\n      <td>0.561000</td>\n    </tr>\n    <tr>\n      <td>4890</td>\n      <td>0.680800</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.583800</td>\n    </tr>\n    <tr>\n      <td>4910</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>4920</td>\n      <td>0.698800</td>\n    </tr>\n    <tr>\n      <td>4930</td>\n      <td>0.499400</td>\n    </tr>\n    <tr>\n      <td>4940</td>\n      <td>0.502300</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>0.554100</td>\n    </tr>\n    <tr>\n      <td>4960</td>\n      <td>0.454000</td>\n    </tr>\n    <tr>\n      <td>4970</td>\n      <td>0.578500</td>\n    </tr>\n    <tr>\n      <td>4980</td>\n      <td>0.533900</td>\n    </tr>\n    <tr>\n      <td>4990</td>\n      <td>0.510900</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.706400</td>\n    </tr>\n    <tr>\n      <td>5010</td>\n      <td>0.653800</td>\n    </tr>\n    <tr>\n      <td>5020</td>\n      <td>0.543800</td>\n    </tr>\n    <tr>\n      <td>5030</td>\n      <td>0.536000</td>\n    </tr>\n    <tr>\n      <td>5040</td>\n      <td>0.591000</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>0.593800</td>\n    </tr>\n    <tr>\n      <td>5060</td>\n      <td>0.519700</td>\n    </tr>\n    <tr>\n      <td>5070</td>\n      <td>0.518900</td>\n    </tr>\n    <tr>\n      <td>5080</td>\n      <td>0.552200</td>\n    </tr>\n    <tr>\n      <td>5090</td>\n      <td>0.652600</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.550700</td>\n    </tr>\n    <tr>\n      <td>5110</td>\n      <td>0.425700</td>\n    </tr>\n    <tr>\n      <td>5120</td>\n      <td>0.677100</td>\n    </tr>\n    <tr>\n      <td>5130</td>\n      <td>0.485400</td>\n    </tr>\n    <tr>\n      <td>5140</td>\n      <td>0.617000</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>0.569600</td>\n    </tr>\n    <tr>\n      <td>5160</td>\n      <td>0.716800</td>\n    </tr>\n    <tr>\n      <td>5170</td>\n      <td>0.715900</td>\n    </tr>\n    <tr>\n      <td>5180</td>\n      <td>0.489000</td>\n    </tr>\n    <tr>\n      <td>5190</td>\n      <td>0.559200</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.671200</td>\n    </tr>\n    <tr>\n      <td>5210</td>\n      <td>0.666000</td>\n    </tr>\n    <tr>\n      <td>5220</td>\n      <td>0.566200</td>\n    </tr>\n    <tr>\n      <td>5230</td>\n      <td>0.571700</td>\n    </tr>\n    <tr>\n      <td>5240</td>\n      <td>0.644400</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>0.534000</td>\n    </tr>\n    <tr>\n      <td>5260</td>\n      <td>0.567200</td>\n    </tr>\n    <tr>\n      <td>5270</td>\n      <td>0.598300</td>\n    </tr>\n    <tr>\n      <td>5280</td>\n      <td>0.789500</td>\n    </tr>\n    <tr>\n      <td>5290</td>\n      <td>0.538100</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.453900</td>\n    </tr>\n    <tr>\n      <td>5310</td>\n      <td>0.690800</td>\n    </tr>\n    <tr>\n      <td>5320</td>\n      <td>0.528500</td>\n    </tr>\n    <tr>\n      <td>5330</td>\n      <td>0.533900</td>\n    </tr>\n    <tr>\n      <td>5340</td>\n      <td>0.448700</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>0.483200</td>\n    </tr>\n    <tr>\n      <td>5360</td>\n      <td>0.628900</td>\n    </tr>\n    <tr>\n      <td>5370</td>\n      <td>0.493700</td>\n    </tr>\n    <tr>\n      <td>5380</td>\n      <td>0.537400</td>\n    </tr>\n    <tr>\n      <td>5390</td>\n      <td>0.517100</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.577500</td>\n    </tr>\n    <tr>\n      <td>5410</td>\n      <td>0.631200</td>\n    </tr>\n    <tr>\n      <td>5420</td>\n      <td>0.525800</td>\n    </tr>\n    <tr>\n      <td>5430</td>\n      <td>0.499400</td>\n    </tr>\n    <tr>\n      <td>5440</td>\n      <td>0.607700</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>0.661900</td>\n    </tr>\n    <tr>\n      <td>5460</td>\n      <td>0.579600</td>\n    </tr>\n    <tr>\n      <td>5470</td>\n      <td>0.542500</td>\n    </tr>\n    <tr>\n      <td>5480</td>\n      <td>0.492700</td>\n    </tr>\n    <tr>\n      <td>5490</td>\n      <td>0.489100</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.616700</td>\n    </tr>\n    <tr>\n      <td>5510</td>\n      <td>0.755500</td>\n    </tr>\n    <tr>\n      <td>5520</td>\n      <td>0.677300</td>\n    </tr>\n    <tr>\n      <td>5530</td>\n      <td>0.719300</td>\n    </tr>\n    <tr>\n      <td>5540</td>\n      <td>0.498600</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>0.510700</td>\n    </tr>\n    <tr>\n      <td>5560</td>\n      <td>0.531800</td>\n    </tr>\n    <tr>\n      <td>5570</td>\n      <td>0.762400</td>\n    </tr>\n    <tr>\n      <td>5580</td>\n      <td>0.575400</td>\n    </tr>\n    <tr>\n      <td>5590</td>\n      <td>0.573800</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.503700</td>\n    </tr>\n    <tr>\n      <td>5610</td>\n      <td>0.634900</td>\n    </tr>\n    <tr>\n      <td>5620</td>\n      <td>0.542700</td>\n    </tr>\n    <tr>\n      <td>5630</td>\n      <td>0.514500</td>\n    </tr>\n    <tr>\n      <td>5640</td>\n      <td>0.649200</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>0.667200</td>\n    </tr>\n    <tr>\n      <td>5660</td>\n      <td>0.493400</td>\n    </tr>\n    <tr>\n      <td>5670</td>\n      <td>0.456600</td>\n    </tr>\n    <tr>\n      <td>5680</td>\n      <td>0.490200</td>\n    </tr>\n    <tr>\n      <td>5690</td>\n      <td>0.650000</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.683700</td>\n    </tr>\n    <tr>\n      <td>5710</td>\n      <td>0.573100</td>\n    </tr>\n    <tr>\n      <td>5720</td>\n      <td>0.595000</td>\n    </tr>\n    <tr>\n      <td>5730</td>\n      <td>0.473800</td>\n    </tr>\n    <tr>\n      <td>5740</td>\n      <td>0.560400</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>0.624300</td>\n    </tr>\n    <tr>\n      <td>5760</td>\n      <td>0.707900</td>\n    </tr>\n    <tr>\n      <td>5770</td>\n      <td>0.480900</td>\n    </tr>\n    <tr>\n      <td>5780</td>\n      <td>0.593400</td>\n    </tr>\n    <tr>\n      <td>5790</td>\n      <td>0.650000</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.526100</td>\n    </tr>\n    <tr>\n      <td>5810</td>\n      <td>0.513400</td>\n    </tr>\n    <tr>\n      <td>5820</td>\n      <td>0.561600</td>\n    </tr>\n    <tr>\n      <td>5830</td>\n      <td>0.472000</td>\n    </tr>\n    <tr>\n      <td>5840</td>\n      <td>0.510700</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>0.547600</td>\n    </tr>\n    <tr>\n      <td>5860</td>\n      <td>0.542600</td>\n    </tr>\n    <tr>\n      <td>5870</td>\n      <td>0.576900</td>\n    </tr>\n    <tr>\n      <td>5880</td>\n      <td>0.598700</td>\n    </tr>\n    <tr>\n      <td>5890</td>\n      <td>0.583400</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.622700</td>\n    </tr>\n    <tr>\n      <td>5910</td>\n      <td>0.772800</td>\n    </tr>\n    <tr>\n      <td>5920</td>\n      <td>0.596000</td>\n    </tr>\n    <tr>\n      <td>5930</td>\n      <td>0.602500</td>\n    </tr>\n    <tr>\n      <td>5940</td>\n      <td>0.501700</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>0.657600</td>\n    </tr>\n    <tr>\n      <td>5960</td>\n      <td>0.632300</td>\n    </tr>\n    <tr>\n      <td>5970</td>\n      <td>0.569800</td>\n    </tr>\n    <tr>\n      <td>5980</td>\n      <td>0.550400</td>\n    </tr>\n    <tr>\n      <td>5990</td>\n      <td>0.598000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.422900</td>\n    </tr>\n    <tr>\n      <td>6010</td>\n      <td>0.633400</td>\n    </tr>\n    <tr>\n      <td>6020</td>\n      <td>0.481600</td>\n    </tr>\n    <tr>\n      <td>6030</td>\n      <td>0.632900</td>\n    </tr>\n    <tr>\n      <td>6040</td>\n      <td>0.462000</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>0.475100</td>\n    </tr>\n    <tr>\n      <td>6060</td>\n      <td>0.592000</td>\n    </tr>\n    <tr>\n      <td>6070</td>\n      <td>0.615400</td>\n    </tr>\n    <tr>\n      <td>6080</td>\n      <td>0.545000</td>\n    </tr>\n    <tr>\n      <td>6090</td>\n      <td>0.508400</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.479000</td>\n    </tr>\n    <tr>\n      <td>6110</td>\n      <td>0.439500</td>\n    </tr>\n    <tr>\n      <td>6120</td>\n      <td>0.605600</td>\n    </tr>\n    <tr>\n      <td>6130</td>\n      <td>0.633000</td>\n    </tr>\n    <tr>\n      <td>6140</td>\n      <td>0.442100</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>0.586300</td>\n    </tr>\n    <tr>\n      <td>6160</td>\n      <td>0.475600</td>\n    </tr>\n    <tr>\n      <td>6170</td>\n      <td>0.593600</td>\n    </tr>\n    <tr>\n      <td>6180</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>6190</td>\n      <td>0.455300</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.492400</td>\n    </tr>\n    <tr>\n      <td>6210</td>\n      <td>0.437700</td>\n    </tr>\n    <tr>\n      <td>6220</td>\n      <td>0.400400</td>\n    </tr>\n    <tr>\n      <td>6230</td>\n      <td>0.573800</td>\n    </tr>\n    <tr>\n      <td>6240</td>\n      <td>0.535200</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>0.536900</td>\n    </tr>\n    <tr>\n      <td>6260</td>\n      <td>0.695900</td>\n    </tr>\n    <tr>\n      <td>6270</td>\n      <td>0.578800</td>\n    </tr>\n    <tr>\n      <td>6280</td>\n      <td>0.502600</td>\n    </tr>\n    <tr>\n      <td>6290</td>\n      <td>0.441500</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.700700</td>\n    </tr>\n    <tr>\n      <td>6310</td>\n      <td>0.529900</td>\n    </tr>\n    <tr>\n      <td>6320</td>\n      <td>0.504600</td>\n    </tr>\n    <tr>\n      <td>6330</td>\n      <td>0.496900</td>\n    </tr>\n    <tr>\n      <td>6340</td>\n      <td>0.472300</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>0.465600</td>\n    </tr>\n    <tr>\n      <td>6360</td>\n      <td>0.474100</td>\n    </tr>\n    <tr>\n      <td>6370</td>\n      <td>0.620100</td>\n    </tr>\n    <tr>\n      <td>6380</td>\n      <td>0.557300</td>\n    </tr>\n    <tr>\n      <td>6390</td>\n      <td>0.383200</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.481000</td>\n    </tr>\n    <tr>\n      <td>6410</td>\n      <td>0.593200</td>\n    </tr>\n    <tr>\n      <td>6420</td>\n      <td>0.565500</td>\n    </tr>\n    <tr>\n      <td>6430</td>\n      <td>0.581700</td>\n    </tr>\n    <tr>\n      <td>6440</td>\n      <td>0.579500</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>0.516700</td>\n    </tr>\n    <tr>\n      <td>6460</td>\n      <td>0.722700</td>\n    </tr>\n    <tr>\n      <td>6470</td>\n      <td>0.434600</td>\n    </tr>\n    <tr>\n      <td>6480</td>\n      <td>0.505900</td>\n    </tr>\n    <tr>\n      <td>6490</td>\n      <td>0.592600</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.483300</td>\n    </tr>\n    <tr>\n      <td>6510</td>\n      <td>0.518900</td>\n    </tr>\n    <tr>\n      <td>6520</td>\n      <td>0.488400</td>\n    </tr>\n    <tr>\n      <td>6530</td>\n      <td>0.505600</td>\n    </tr>\n    <tr>\n      <td>6540</td>\n      <td>0.549700</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>0.532900</td>\n    </tr>\n    <tr>\n      <td>6560</td>\n      <td>0.425900</td>\n    </tr>\n    <tr>\n      <td>6570</td>\n      <td>0.595500</td>\n    </tr>\n    <tr>\n      <td>6580</td>\n      <td>0.487000</td>\n    </tr>\n    <tr>\n      <td>6590</td>\n      <td>0.510700</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.561100</td>\n    </tr>\n    <tr>\n      <td>6610</td>\n      <td>0.337600</td>\n    </tr>\n    <tr>\n      <td>6620</td>\n      <td>0.561000</td>\n    </tr>\n    <tr>\n      <td>6630</td>\n      <td>0.459500</td>\n    </tr>\n    <tr>\n      <td>6640</td>\n      <td>0.598400</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>0.441600</td>\n    </tr>\n    <tr>\n      <td>6660</td>\n      <td>0.564900</td>\n    </tr>\n    <tr>\n      <td>6670</td>\n      <td>0.501400</td>\n    </tr>\n    <tr>\n      <td>6680</td>\n      <td>0.549100</td>\n    </tr>\n    <tr>\n      <td>6690</td>\n      <td>0.590500</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.442300</td>\n    </tr>\n    <tr>\n      <td>6710</td>\n      <td>0.542300</td>\n    </tr>\n    <tr>\n      <td>6720</td>\n      <td>0.485600</td>\n    </tr>\n    <tr>\n      <td>6730</td>\n      <td>0.444200</td>\n    </tr>\n    <tr>\n      <td>6740</td>\n      <td>0.541900</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>0.483900</td>\n    </tr>\n    <tr>\n      <td>6760</td>\n      <td>0.525400</td>\n    </tr>\n    <tr>\n      <td>6770</td>\n      <td>0.490500</td>\n    </tr>\n    <tr>\n      <td>6780</td>\n      <td>0.486900</td>\n    </tr>\n    <tr>\n      <td>6790</td>\n      <td>0.640500</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.520800</td>\n    </tr>\n    <tr>\n      <td>6810</td>\n      <td>0.505800</td>\n    </tr>\n    <tr>\n      <td>6820</td>\n      <td>0.553200</td>\n    </tr>\n    <tr>\n      <td>6830</td>\n      <td>0.561600</td>\n    </tr>\n    <tr>\n      <td>6840</td>\n      <td>0.523600</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>0.525300</td>\n    </tr>\n    <tr>\n      <td>6860</td>\n      <td>0.470300</td>\n    </tr>\n    <tr>\n      <td>6870</td>\n      <td>0.531200</td>\n    </tr>\n    <tr>\n      <td>6880</td>\n      <td>0.597200</td>\n    </tr>\n    <tr>\n      <td>6890</td>\n      <td>0.564100</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.484900</td>\n    </tr>\n    <tr>\n      <td>6910</td>\n      <td>0.550100</td>\n    </tr>\n    <tr>\n      <td>6920</td>\n      <td>0.571500</td>\n    </tr>\n    <tr>\n      <td>6930</td>\n      <td>0.517700</td>\n    </tr>\n    <tr>\n      <td>6940</td>\n      <td>0.561900</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>0.566100</td>\n    </tr>\n    <tr>\n      <td>6960</td>\n      <td>0.561400</td>\n    </tr>\n    <tr>\n      <td>6970</td>\n      <td>0.372000</td>\n    </tr>\n    <tr>\n      <td>6980</td>\n      <td>0.517100</td>\n    </tr>\n    <tr>\n      <td>6990</td>\n      <td>0.415100</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.531300</td>\n    </tr>\n    <tr>\n      <td>7010</td>\n      <td>0.436800</td>\n    </tr>\n    <tr>\n      <td>7020</td>\n      <td>0.615300</td>\n    </tr>\n    <tr>\n      <td>7030</td>\n      <td>0.512600</td>\n    </tr>\n    <tr>\n      <td>7040</td>\n      <td>0.454700</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>0.452600</td>\n    </tr>\n    <tr>\n      <td>7060</td>\n      <td>0.527800</td>\n    </tr>\n    <tr>\n      <td>7070</td>\n      <td>0.543300</td>\n    </tr>\n    <tr>\n      <td>7080</td>\n      <td>0.553300</td>\n    </tr>\n    <tr>\n      <td>7090</td>\n      <td>0.456500</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.532700</td>\n    </tr>\n    <tr>\n      <td>7110</td>\n      <td>0.620200</td>\n    </tr>\n    <tr>\n      <td>7120</td>\n      <td>0.394000</td>\n    </tr>\n    <tr>\n      <td>7130</td>\n      <td>0.576400</td>\n    </tr>\n    <tr>\n      <td>7140</td>\n      <td>0.611600</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>0.508500</td>\n    </tr>\n    <tr>\n      <td>7160</td>\n      <td>0.553100</td>\n    </tr>\n    <tr>\n      <td>7170</td>\n      <td>0.561400</td>\n    </tr>\n    <tr>\n      <td>7180</td>\n      <td>0.573300</td>\n    </tr>\n    <tr>\n      <td>7190</td>\n      <td>0.624200</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.477100</td>\n    </tr>\n    <tr>\n      <td>7210</td>\n      <td>0.612200</td>\n    </tr>\n    <tr>\n      <td>7220</td>\n      <td>0.630300</td>\n    </tr>\n    <tr>\n      <td>7230</td>\n      <td>0.540900</td>\n    </tr>\n    <tr>\n      <td>7240</td>\n      <td>0.525200</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>0.624000</td>\n    </tr>\n    <tr>\n      <td>7260</td>\n      <td>0.450500</td>\n    </tr>\n    <tr>\n      <td>7270</td>\n      <td>0.660200</td>\n    </tr>\n    <tr>\n      <td>7280</td>\n      <td>0.578800</td>\n    </tr>\n    <tr>\n      <td>7290</td>\n      <td>0.712600</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.418500</td>\n    </tr>\n    <tr>\n      <td>7310</td>\n      <td>0.539600</td>\n    </tr>\n    <tr>\n      <td>7320</td>\n      <td>0.544400</td>\n    </tr>\n    <tr>\n      <td>7330</td>\n      <td>0.440900</td>\n    </tr>\n    <tr>\n      <td>7340</td>\n      <td>0.640000</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>0.471800</td>\n    </tr>\n    <tr>\n      <td>7360</td>\n      <td>0.537900</td>\n    </tr>\n    <tr>\n      <td>7370</td>\n      <td>0.578700</td>\n    </tr>\n    <tr>\n      <td>7380</td>\n      <td>0.523300</td>\n    </tr>\n    <tr>\n      <td>7390</td>\n      <td>0.562900</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.506500</td>\n    </tr>\n    <tr>\n      <td>7410</td>\n      <td>0.528600</td>\n    </tr>\n    <tr>\n      <td>7420</td>\n      <td>0.579800</td>\n    </tr>\n    <tr>\n      <td>7430</td>\n      <td>0.517000</td>\n    </tr>\n    <tr>\n      <td>7440</td>\n      <td>0.471300</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>0.470100</td>\n    </tr>\n    <tr>\n      <td>7460</td>\n      <td>0.594100</td>\n    </tr>\n    <tr>\n      <td>7470</td>\n      <td>0.407600</td>\n    </tr>\n    <tr>\n      <td>7480</td>\n      <td>0.399000</td>\n    </tr>\n    <tr>\n      <td>7490</td>\n      <td>0.510100</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.561900</td>\n    </tr>\n    <tr>\n      <td>7510</td>\n      <td>0.422500</td>\n    </tr>\n    <tr>\n      <td>7520</td>\n      <td>0.501500</td>\n    </tr>\n    <tr>\n      <td>7530</td>\n      <td>0.561600</td>\n    </tr>\n    <tr>\n      <td>7540</td>\n      <td>0.500300</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>0.539400</td>\n    </tr>\n    <tr>\n      <td>7560</td>\n      <td>0.493500</td>\n    </tr>\n    <tr>\n      <td>7570</td>\n      <td>0.551600</td>\n    </tr>\n    <tr>\n      <td>7580</td>\n      <td>0.568600</td>\n    </tr>\n    <tr>\n      <td>7590</td>\n      <td>0.537000</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.570000</td>\n    </tr>\n    <tr>\n      <td>7610</td>\n      <td>0.477700</td>\n    </tr>\n    <tr>\n      <td>7620</td>\n      <td>0.483700</td>\n    </tr>\n    <tr>\n      <td>7630</td>\n      <td>0.472500</td>\n    </tr>\n    <tr>\n      <td>7640</td>\n      <td>0.513200</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>0.626500</td>\n    </tr>\n    <tr>\n      <td>7660</td>\n      <td>0.451000</td>\n    </tr>\n    <tr>\n      <td>7670</td>\n      <td>0.509100</td>\n    </tr>\n    <tr>\n      <td>7680</td>\n      <td>0.582100</td>\n    </tr>\n    <tr>\n      <td>7690</td>\n      <td>0.600700</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.511900</td>\n    </tr>\n    <tr>\n      <td>7710</td>\n      <td>0.433900</td>\n    </tr>\n    <tr>\n      <td>7720</td>\n      <td>0.481400</td>\n    </tr>\n    <tr>\n      <td>7730</td>\n      <td>0.636500</td>\n    </tr>\n    <tr>\n      <td>7740</td>\n      <td>0.441700</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>7760</td>\n      <td>0.496400</td>\n    </tr>\n    <tr>\n      <td>7770</td>\n      <td>0.542900</td>\n    </tr>\n    <tr>\n      <td>7780</td>\n      <td>0.526800</td>\n    </tr>\n    <tr>\n      <td>7790</td>\n      <td>0.396000</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.537900</td>\n    </tr>\n    <tr>\n      <td>7810</td>\n      <td>0.556500</td>\n    </tr>\n    <tr>\n      <td>7820</td>\n      <td>0.498300</td>\n    </tr>\n    <tr>\n      <td>7830</td>\n      <td>0.465500</td>\n    </tr>\n    <tr>\n      <td>7840</td>\n      <td>0.461800</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>0.455400</td>\n    </tr>\n    <tr>\n      <td>7860</td>\n      <td>0.551200</td>\n    </tr>\n    <tr>\n      <td>7870</td>\n      <td>0.505100</td>\n    </tr>\n    <tr>\n      <td>7880</td>\n      <td>0.459000</td>\n    </tr>\n    <tr>\n      <td>7890</td>\n      <td>0.424700</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.399500</td>\n    </tr>\n    <tr>\n      <td>7910</td>\n      <td>0.553300</td>\n    </tr>\n    <tr>\n      <td>7920</td>\n      <td>0.554000</td>\n    </tr>\n    <tr>\n      <td>7930</td>\n      <td>0.471700</td>\n    </tr>\n    <tr>\n      <td>7940</td>\n      <td>0.450100</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>0.530800</td>\n    </tr>\n    <tr>\n      <td>7960</td>\n      <td>0.404800</td>\n    </tr>\n    <tr>\n      <td>7970</td>\n      <td>0.617500</td>\n    </tr>\n    <tr>\n      <td>7980</td>\n      <td>0.366400</td>\n    </tr>\n    <tr>\n      <td>7990</td>\n      <td>0.482800</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.500300</td>\n    </tr>\n    <tr>\n      <td>8010</td>\n      <td>0.444900</td>\n    </tr>\n    <tr>\n      <td>8020</td>\n      <td>0.472300</td>\n    </tr>\n    <tr>\n      <td>8030</td>\n      <td>0.574600</td>\n    </tr>\n    <tr>\n      <td>8040</td>\n      <td>0.458800</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>0.620700</td>\n    </tr>\n    <tr>\n      <td>8060</td>\n      <td>0.459400</td>\n    </tr>\n    <tr>\n      <td>8070</td>\n      <td>0.697800</td>\n    </tr>\n    <tr>\n      <td>8080</td>\n      <td>0.573800</td>\n    </tr>\n    <tr>\n      <td>8090</td>\n      <td>0.422400</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.439800</td>\n    </tr>\n    <tr>\n      <td>8110</td>\n      <td>0.448900</td>\n    </tr>\n    <tr>\n      <td>8120</td>\n      <td>0.431000</td>\n    </tr>\n    <tr>\n      <td>8130</td>\n      <td>0.520200</td>\n    </tr>\n    <tr>\n      <td>8140</td>\n      <td>0.641500</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>0.483200</td>\n    </tr>\n    <tr>\n      <td>8160</td>\n      <td>0.408300</td>\n    </tr>\n    <tr>\n      <td>8170</td>\n      <td>0.583300</td>\n    </tr>\n    <tr>\n      <td>8180</td>\n      <td>0.413800</td>\n    </tr>\n    <tr>\n      <td>8190</td>\n      <td>0.513000</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.528100</td>\n    </tr>\n    <tr>\n      <td>8210</td>\n      <td>0.442000</td>\n    </tr>\n    <tr>\n      <td>8220</td>\n      <td>0.418200</td>\n    </tr>\n    <tr>\n      <td>8230</td>\n      <td>0.569500</td>\n    </tr>\n    <tr>\n      <td>8240</td>\n      <td>0.509200</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>0.441300</td>\n    </tr>\n    <tr>\n      <td>8260</td>\n      <td>0.468500</td>\n    </tr>\n    <tr>\n      <td>8270</td>\n      <td>0.578800</td>\n    </tr>\n    <tr>\n      <td>8280</td>\n      <td>0.484800</td>\n    </tr>\n    <tr>\n      <td>8290</td>\n      <td>0.422700</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.459300</td>\n    </tr>\n    <tr>\n      <td>8310</td>\n      <td>0.587100</td>\n    </tr>\n    <tr>\n      <td>8320</td>\n      <td>0.488100</td>\n    </tr>\n    <tr>\n      <td>8330</td>\n      <td>0.476700</td>\n    </tr>\n    <tr>\n      <td>8340</td>\n      <td>0.434100</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>0.448000</td>\n    </tr>\n    <tr>\n      <td>8360</td>\n      <td>0.578200</td>\n    </tr>\n    <tr>\n      <td>8370</td>\n      <td>0.609700</td>\n    </tr>\n    <tr>\n      <td>8380</td>\n      <td>0.549700</td>\n    </tr>\n    <tr>\n      <td>8390</td>\n      <td>0.428700</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.533500</td>\n    </tr>\n    <tr>\n      <td>8410</td>\n      <td>0.512800</td>\n    </tr>\n    <tr>\n      <td>8420</td>\n      <td>0.631200</td>\n    </tr>\n    <tr>\n      <td>8430</td>\n      <td>0.563400</td>\n    </tr>\n    <tr>\n      <td>8440</td>\n      <td>0.453300</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>0.475800</td>\n    </tr>\n    <tr>\n      <td>8460</td>\n      <td>0.570400</td>\n    </tr>\n    <tr>\n      <td>8470</td>\n      <td>0.502800</td>\n    </tr>\n    <tr>\n      <td>8480</td>\n      <td>0.463500</td>\n    </tr>\n    <tr>\n      <td>8490</td>\n      <td>0.573600</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.605200</td>\n    </tr>\n    <tr>\n      <td>8510</td>\n      <td>0.460600</td>\n    </tr>\n    <tr>\n      <td>8520</td>\n      <td>0.455400</td>\n    </tr>\n    <tr>\n      <td>8530</td>\n      <td>0.418600</td>\n    </tr>\n    <tr>\n      <td>8540</td>\n      <td>0.515300</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>0.605800</td>\n    </tr>\n    <tr>\n      <td>8560</td>\n      <td>0.570000</td>\n    </tr>\n    <tr>\n      <td>8570</td>\n      <td>0.570800</td>\n    </tr>\n    <tr>\n      <td>8580</td>\n      <td>0.507600</td>\n    </tr>\n    <tr>\n      <td>8590</td>\n      <td>0.503300</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.488600</td>\n    </tr>\n    <tr>\n      <td>8610</td>\n      <td>0.587700</td>\n    </tr>\n    <tr>\n      <td>8620</td>\n      <td>0.476000</td>\n    </tr>\n    <tr>\n      <td>8630</td>\n      <td>0.462500</td>\n    </tr>\n    <tr>\n      <td>8640</td>\n      <td>0.561700</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>0.551800</td>\n    </tr>\n    <tr>\n      <td>8660</td>\n      <td>0.608300</td>\n    </tr>\n    <tr>\n      <td>8670</td>\n      <td>0.488800</td>\n    </tr>\n    <tr>\n      <td>8680</td>\n      <td>0.456900</td>\n    </tr>\n    <tr>\n      <td>8690</td>\n      <td>0.527600</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.564600</td>\n    </tr>\n    <tr>\n      <td>8710</td>\n      <td>0.530300</td>\n    </tr>\n    <tr>\n      <td>8720</td>\n      <td>0.597200</td>\n    </tr>\n    <tr>\n      <td>8730</td>\n      <td>0.536300</td>\n    </tr>\n    <tr>\n      <td>8740</td>\n      <td>0.577000</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>0.394500</td>\n    </tr>\n    <tr>\n      <td>8760</td>\n      <td>0.619500</td>\n    </tr>\n    <tr>\n      <td>8770</td>\n      <td>0.424200</td>\n    </tr>\n    <tr>\n      <td>8780</td>\n      <td>0.511200</td>\n    </tr>\n    <tr>\n      <td>8790</td>\n      <td>0.540900</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.520400</td>\n    </tr>\n    <tr>\n      <td>8810</td>\n      <td>0.599200</td>\n    </tr>\n    <tr>\n      <td>8820</td>\n      <td>0.478700</td>\n    </tr>\n    <tr>\n      <td>8830</td>\n      <td>0.736400</td>\n    </tr>\n    <tr>\n      <td>8840</td>\n      <td>0.419800</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>0.514000</td>\n    </tr>\n    <tr>\n      <td>8860</td>\n      <td>0.396900</td>\n    </tr>\n    <tr>\n      <td>8870</td>\n      <td>0.500300</td>\n    </tr>\n    <tr>\n      <td>8880</td>\n      <td>0.461600</td>\n    </tr>\n    <tr>\n      <td>8890</td>\n      <td>0.569400</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.553400</td>\n    </tr>\n    <tr>\n      <td>8910</td>\n      <td>0.555500</td>\n    </tr>\n    <tr>\n      <td>8920</td>\n      <td>0.503200</td>\n    </tr>\n    <tr>\n      <td>8930</td>\n      <td>0.562400</td>\n    </tr>\n    <tr>\n      <td>8940</td>\n      <td>0.508700</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>0.413100</td>\n    </tr>\n    <tr>\n      <td>8960</td>\n      <td>0.461800</td>\n    </tr>\n    <tr>\n      <td>8970</td>\n      <td>0.464400</td>\n    </tr>\n    <tr>\n      <td>8980</td>\n      <td>0.406100</td>\n    </tr>\n    <tr>\n      <td>8990</td>\n      <td>0.515700</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.392000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Training complete!\n✅ Model and label encoder saved.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:25:05.234535Z","iopub.execute_input":"2025-05-05T15:25:05.234847Z","iopub.status.idle":"2025-05-05T15:25:14.867731Z","shell.execute_reply.started":"2025-05-05T15:25:05.234816Z","shell.execute_reply":"2025-05-05T15:25:14.866946Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"pip install rouge-score bert-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:27:17.492855Z","iopub.execute_input":"2025-05-05T15:27:17.493188Z","iopub.status.idle":"2025-05-05T15:28:49.727001Z","shell.execute_reply.started":"2025-05-05T15:27:17.493160Z","shell.execute_reply":"2025-05-05T15:28:49.726124Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nCollecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert-score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom bert_score import score as bert_score\nimport torch\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm  # Import tqdm for progress bar\n\n# Load model and tokenizer\nmodel_path = './bert-text-classifier'\ntokenizer = BertTokenizer.from_pretrained(model_path)\nmodel = BertForSequenceClassification.from_pretrained(model_path)\nmodel.eval()\n\n# Load label encoder\nwith open('./label_encoder.pkl', 'rb') as f:\n    le = pickle.load(f)\n\n# Load validation data\nval_texts = val_dataset['input_ids']\nval_attention = val_dataset['attention_mask']\nval_labels = val_dataset['label']\n\n# Get predictions\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\npreds = []\nreferences = []\n\n# Use tqdm for progress bar\nwith torch.no_grad():\n    for i in tqdm(range(0, len(val_texts), 8), desc=\"Evaluating...\"):  # batch size = 8\n        input_ids = val_texts[i:i+8].to(device)\n        attention_mask = val_attention[i:i+8].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        batch_preds = torch.argmax(logits, dim=-1).cpu().tolist()\n        preds.extend(batch_preds)\n        references.extend(val_labels[i:i+8].tolist())\n\n# Decode predicted and reference labels to strings\npred_labels = le.inverse_transform(preds)\ntrue_labels = le.inverse_transform(references)\n\n# Create dummy texts for BERTScore (class labels as text)\npred_texts = [str(label) for label in pred_labels]\nref_texts = [str(label) for label in true_labels]\n\n# Calculate BERTScore\nP, R, F1 = bert_score(pred_texts, ref_texts, lang=\"en\", model_type=\"bert-base-uncased\")\n\n# Output results\nprint(f\"\\n📊 BERTScore Evaluation:\")\nprint(f\"Precision: {P.mean().item():.4f}\")\nprint(f\"Recall:    {R.mean().item():.4f}\")\nprint(f\"F1 Score:  {F1.mean().item():.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:48:38.648716Z","iopub.execute_input":"2025-05-05T15:48:38.649076Z","iopub.status.idle":"2025-05-05T15:54:45.045946Z","shell.execute_reply.started":"2025-05-05T15:48:38.649041Z","shell.execute_reply":"2025-05-05T15:54:45.045102Z"}},"outputs":[{"name":"stderr","text":"Evaluating...: 100%|██████████| 1500/1500 [06:03<00:00,  4.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📊 BERTScore Evaluation:\nPrecision: 0.8901\nRecall:    0.8977\nF1 Score:  0.8922\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"from rouge_score import rouge_scorer\nfrom sklearn.metrics import classification_report\n\n# Create scorer for ROUGE-1, ROUGE-2, and ROUGE-L\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# Compute ROUGE scores between predicted and true labels (as strings)\nrouge1_scores = []\nrouge2_scores = []\nrougeL_scores = []\n\nfor pred, ref in zip(pred_labels, true_labels):\n    scores = scorer.score(ref, pred)\n    rouge1_scores.append(scores['rouge1'].fmeasure)\n    rouge2_scores.append(scores['rouge2'].fmeasure)\n    rougeL_scores.append(scores['rougeL'].fmeasure)\n\n# Calculate average ROUGE scores\navg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\navg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\navg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n\n# Output\nprint(\"\\n📊 ROUGE Score Evaluation:\")\nprint(f\"ROUGE-1: {avg_rouge1:.4f}\")\nprint(f\"ROUGE-2: {avg_rouge2:.4f}\")\nprint(f\"ROUGE-L : {avg_rougeL:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T15:56:22.185529Z","iopub.execute_input":"2025-05-05T15:56:22.185906Z","iopub.status.idle":"2025-05-05T15:56:23.329283Z","shell.execute_reply.started":"2025-05-05T15:56:22.185877Z","shell.execute_reply":"2025-05-05T15:56:23.328563Z"}},"outputs":[{"name":"stdout","text":"\n📊 ROUGE Score Evaluation:\nROUGE-1: 0.8033\nROUGE-2: 0.4437\nROUGE-L : 0.8033\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import XLNetTokenizer, XLNetForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport pickle\n\n# Disable WandB in Kaggle\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Load data\ndata = pd.read_csv('/kaggle/input/balanced-augumented-data/balanced_augmented_data.csv')\n\n# Encode labels\nle = LabelEncoder()\ndata['label'] = le.fit_transform(data['medical_specialty'])\n\n# Split into train and validation\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    data['transcription'], data['label'], test_size=0.2, stratify=data['label'], random_state=42\n)\n\n# Load tokenizer and model\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\nmodel = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=len(le.classes_))\n\n# Tokenization function\ndef tokenize_function(example):\n    return tokenizer(example['transcription'], truncation=True, padding='max_length', max_length=512)\n\n# Create Hugging Face Datasets\ntrain_dataset = Dataset.from_dict({'transcription': train_texts.tolist(), 'label': train_labels.tolist()})\nval_dataset = Dataset.from_dict({'transcription': val_texts.tolist(), 'label': val_labels.tolist()})\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n\n# Set format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./xlnet-results\",\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# Define Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model\nprint(\"🚀 Starting XLNet training...\")\ntrainer.train()\nprint(\"✅ XLNet training complete!\")\n\n# Save model and tokenizer\nmodel.save_pretrained('./xlnet-text-classifier')\ntokenizer.save_pretrained('./xlnet-text-classifier')\n\n# Save label encoder\nwith open('./xlnet_label_encoder.pkl', 'wb') as f:\n    pickle.dump(le, f)\n\nprint(\"✅ Model, tokenizer, and label encoder saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:15:37.075428Z","iopub.execute_input":"2025-05-05T17:15:37.075748Z","iopub.status.idle":"2025-05-05T20:08:51.252023Z","shell.execute_reply.started":"2025-05-05T17:15:37.075723Z","shell.execute_reply":"2025-05-05T20:08:51.251363Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/48000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bfc018b5eec4b60ac07a9523ce0b493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1ef37220ff84168be91861ec0ebe2be"}},"metadata":{}},{"name":"stdout","text":"🚀 Starting XLNet training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6000/6000 2:51:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.799100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>3.701200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.796000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.769300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.722800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.782300</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>3.712500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.763000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>3.809600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.783200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>3.780100</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.723600</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>3.709800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>3.722900</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>3.764600</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>3.772100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>3.781100</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>3.773000</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>3.673400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.710300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>3.734900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>3.733500</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>3.745300</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>3.701400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>3.718200</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>3.746500</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>3.700400</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>3.719500</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>3.691500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.733600</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>3.698200</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>3.778500</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>3.704500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>3.742300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>3.735300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>3.740100</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>3.710300</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>3.731700</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>3.683500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.758800</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>3.696800</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>3.771600</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>3.685300</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>3.707400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>3.704900</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>3.670500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>3.701200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>3.764700</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>3.713200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.746300</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>3.690400</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>3.676500</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>3.757400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>3.723800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>3.688900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>3.735500</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>3.677000</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>3.714200</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>3.746300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.762300</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>3.688700</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>3.760000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>3.745300</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>3.712500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>3.694000</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>3.706500</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>3.742500</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>3.677400</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>3.778200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.718100</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>3.719000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>3.666100</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>3.728900</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>3.709900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>3.756700</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>3.769000</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>3.695900</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>3.716100</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>3.705000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.737600</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>3.749400</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>3.712000</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>3.731500</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>3.717500</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>3.709400</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>3.737900</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>3.717400</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>3.758600</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>3.726400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.729700</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>3.713900</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>3.720700</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>3.736800</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>3.758800</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>3.714500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>3.747300</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>3.739600</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>3.685200</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>3.796700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.750000</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>3.688200</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>3.676500</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>3.699500</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>3.699700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>3.752200</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>3.744700</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>3.712100</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>3.716200</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>3.711000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.676200</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>3.703000</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>3.728000</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>3.703200</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>3.749000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>3.719500</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>3.729700</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>3.772500</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>3.702200</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>3.698200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.744800</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>3.706700</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>3.677800</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>3.678300</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>3.685700</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>3.704700</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>3.707200</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>3.763100</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>3.712200</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>3.768300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.718500</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>3.703400</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>3.719300</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>3.729600</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>3.718300</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>3.749000</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>3.714500</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>3.751900</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>3.707400</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>3.731800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.725400</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>3.710400</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>3.763400</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>3.716200</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>3.728600</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>3.720100</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>3.700400</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>3.673900</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>3.741800</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>3.705400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.708300</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>3.681300</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>3.709300</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>3.715500</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>3.664100</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>3.729000</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>3.694200</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>3.686500</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>3.678700</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>3.753100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.739000</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>3.680800</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>3.696700</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>3.701300</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>3.686700</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>3.710000</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>3.720500</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>3.720200</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>3.717300</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>3.724200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>3.689100</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>3.679800</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>3.723600</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>3.699500</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>3.687100</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>3.693300</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>3.749000</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>3.733700</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>3.718000</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>3.707700</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.711000</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>3.699600</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>3.721200</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>3.710100</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>3.724000</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>3.701500</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>3.683200</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>3.702200</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>3.694400</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>3.710600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>3.702200</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>3.773100</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>3.727900</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>3.651800</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>3.702200</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>3.713500</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>3.731200</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>3.715300</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>3.726100</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>3.724100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.703800</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>3.656300</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>3.734000</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>3.691800</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>3.751600</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>3.649400</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>3.703600</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>3.767100</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>3.753900</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>3.670300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>3.680300</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>3.717700</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>3.723700</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>3.712900</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>3.697800</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>3.714100</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>3.663800</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>3.673000</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>3.701700</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>3.717000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>3.729300</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>3.712000</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>3.686100</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>3.687100</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>3.693600</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>3.738700</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>3.706700</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>3.723700</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>3.701000</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>3.723300</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>3.715000</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>3.726700</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>3.722700</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>3.741100</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>3.680900</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>3.739000</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>3.698300</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>3.717900</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>3.709800</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>3.721600</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>3.695900</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>3.696400</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>3.668300</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>3.712200</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>3.699200</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>3.694200</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>3.699900</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>3.684400</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>3.717800</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>3.723500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.703500</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>3.747800</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>3.700100</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>3.722900</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>3.746400</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>3.742600</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>3.720300</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>3.715300</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>3.723200</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>3.714100</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>3.700500</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>3.721700</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>3.712400</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>3.692300</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>3.681600</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>3.705700</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>3.686200</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>3.730100</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>3.700800</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>3.741400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>3.667500</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>3.697900</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>3.734300</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>3.666200</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>3.697500</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>3.763300</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>3.682800</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>3.686300</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>3.700700</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>3.714100</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>3.717000</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>3.704800</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>3.705800</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>3.717400</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>3.735900</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>3.686800</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>3.692700</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>3.702400</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>3.699800</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>3.710900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>3.702900</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>3.703000</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>3.712200</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>3.693300</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>3.645900</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>3.692200</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>3.697300</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>3.705600</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>3.716700</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>3.656600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.716900</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>3.702600</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>3.684000</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>3.691600</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>3.736500</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>3.706800</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>3.683300</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>3.657200</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>3.701300</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>3.706000</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>3.700700</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>3.694200</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>3.697000</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>3.702500</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>3.707100</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>3.697100</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>3.707800</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>3.712800</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>3.697500</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>3.714000</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>3.691000</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>3.704400</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>3.714600</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>3.695700</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>3.710700</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>3.703400</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>3.706600</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>3.686600</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>3.723100</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>3.669600</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>3.713200</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>3.703600</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>3.687500</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>3.699600</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>3.703900</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>3.717600</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>3.682000</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>3.696200</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>3.698200</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>3.694800</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>3.717500</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>3.667400</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>3.689300</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>3.702500</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>3.680000</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>3.704900</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>3.696700</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>3.682600</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>3.705500</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>3.693600</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.722500</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>3.702200</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>3.713000</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>3.710400</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>3.700100</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>3.717400</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>3.671900</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>3.723400</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>3.704300</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>3.708500</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>3.708200</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>3.693100</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>3.686300</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>3.699700</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>3.719000</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>3.708100</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>3.692500</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>3.690600</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>3.700500</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>3.699500</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>3.687400</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>3.715900</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>3.707100</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>3.684200</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>3.731900</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>3.725800</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>3.689100</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>3.689500</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>3.691400</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>3.711600</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>3.684000</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>3.706300</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>3.716500</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>3.701400</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>3.694800</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>3.708000</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>3.698500</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>3.714200</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>3.698300</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>3.676400</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>3.707200</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>3.687200</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>3.704900</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>3.698900</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>3.710000</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>3.696500</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>3.667400</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>3.679500</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>3.702600</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>3.690900</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.704900</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>3.707200</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>3.683200</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>3.697400</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>3.701700</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>3.710700</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>3.728800</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>3.693000</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>3.710300</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>3.703000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>3.717700</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>3.708000</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>3.713900</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>3.703900</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>3.709100</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>3.687800</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>3.691000</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>3.685800</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>3.689000</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>3.705700</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>3.695300</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>3.710800</td>\n    </tr>\n    <tr>\n      <td>4220</td>\n      <td>3.706200</td>\n    </tr>\n    <tr>\n      <td>4230</td>\n      <td>3.701500</td>\n    </tr>\n    <tr>\n      <td>4240</td>\n      <td>3.688700</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>3.679700</td>\n    </tr>\n    <tr>\n      <td>4260</td>\n      <td>3.701900</td>\n    </tr>\n    <tr>\n      <td>4270</td>\n      <td>3.686600</td>\n    </tr>\n    <tr>\n      <td>4280</td>\n      <td>3.692600</td>\n    </tr>\n    <tr>\n      <td>4290</td>\n      <td>3.672300</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>3.697700</td>\n    </tr>\n    <tr>\n      <td>4310</td>\n      <td>3.715100</td>\n    </tr>\n    <tr>\n      <td>4320</td>\n      <td>3.667000</td>\n    </tr>\n    <tr>\n      <td>4330</td>\n      <td>3.700800</td>\n    </tr>\n    <tr>\n      <td>4340</td>\n      <td>3.676500</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>3.669400</td>\n    </tr>\n    <tr>\n      <td>4360</td>\n      <td>3.706000</td>\n    </tr>\n    <tr>\n      <td>4370</td>\n      <td>3.709200</td>\n    </tr>\n    <tr>\n      <td>4380</td>\n      <td>3.690000</td>\n    </tr>\n    <tr>\n      <td>4390</td>\n      <td>3.693800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>3.683500</td>\n    </tr>\n    <tr>\n      <td>4410</td>\n      <td>3.685800</td>\n    </tr>\n    <tr>\n      <td>4420</td>\n      <td>3.700100</td>\n    </tr>\n    <tr>\n      <td>4430</td>\n      <td>3.678500</td>\n    </tr>\n    <tr>\n      <td>4440</td>\n      <td>3.701800</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>3.686500</td>\n    </tr>\n    <tr>\n      <td>4460</td>\n      <td>3.677200</td>\n    </tr>\n    <tr>\n      <td>4470</td>\n      <td>3.682500</td>\n    </tr>\n    <tr>\n      <td>4480</td>\n      <td>3.687500</td>\n    </tr>\n    <tr>\n      <td>4490</td>\n      <td>3.701200</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>3.697400</td>\n    </tr>\n    <tr>\n      <td>4510</td>\n      <td>3.690800</td>\n    </tr>\n    <tr>\n      <td>4520</td>\n      <td>3.696300</td>\n    </tr>\n    <tr>\n      <td>4530</td>\n      <td>3.724100</td>\n    </tr>\n    <tr>\n      <td>4540</td>\n      <td>3.694600</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>3.706100</td>\n    </tr>\n    <tr>\n      <td>4560</td>\n      <td>3.730000</td>\n    </tr>\n    <tr>\n      <td>4570</td>\n      <td>3.691300</td>\n    </tr>\n    <tr>\n      <td>4580</td>\n      <td>3.710000</td>\n    </tr>\n    <tr>\n      <td>4590</td>\n      <td>3.693700</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>3.702300</td>\n    </tr>\n    <tr>\n      <td>4610</td>\n      <td>3.701300</td>\n    </tr>\n    <tr>\n      <td>4620</td>\n      <td>3.688600</td>\n    </tr>\n    <tr>\n      <td>4630</td>\n      <td>3.708000</td>\n    </tr>\n    <tr>\n      <td>4640</td>\n      <td>3.721200</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>3.681500</td>\n    </tr>\n    <tr>\n      <td>4660</td>\n      <td>3.692500</td>\n    </tr>\n    <tr>\n      <td>4670</td>\n      <td>3.697200</td>\n    </tr>\n    <tr>\n      <td>4680</td>\n      <td>3.687200</td>\n    </tr>\n    <tr>\n      <td>4690</td>\n      <td>3.710200</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>3.679200</td>\n    </tr>\n    <tr>\n      <td>4710</td>\n      <td>3.694900</td>\n    </tr>\n    <tr>\n      <td>4720</td>\n      <td>3.672200</td>\n    </tr>\n    <tr>\n      <td>4730</td>\n      <td>3.699300</td>\n    </tr>\n    <tr>\n      <td>4740</td>\n      <td>3.688800</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>3.706000</td>\n    </tr>\n    <tr>\n      <td>4760</td>\n      <td>3.681000</td>\n    </tr>\n    <tr>\n      <td>4770</td>\n      <td>3.688600</td>\n    </tr>\n    <tr>\n      <td>4780</td>\n      <td>3.691600</td>\n    </tr>\n    <tr>\n      <td>4790</td>\n      <td>3.681600</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>3.713300</td>\n    </tr>\n    <tr>\n      <td>4810</td>\n      <td>3.686900</td>\n    </tr>\n    <tr>\n      <td>4820</td>\n      <td>3.691900</td>\n    </tr>\n    <tr>\n      <td>4830</td>\n      <td>3.664100</td>\n    </tr>\n    <tr>\n      <td>4840</td>\n      <td>3.715000</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>3.696100</td>\n    </tr>\n    <tr>\n      <td>4860</td>\n      <td>3.701700</td>\n    </tr>\n    <tr>\n      <td>4870</td>\n      <td>3.694600</td>\n    </tr>\n    <tr>\n      <td>4880</td>\n      <td>3.715800</td>\n    </tr>\n    <tr>\n      <td>4890</td>\n      <td>3.702600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>3.693100</td>\n    </tr>\n    <tr>\n      <td>4910</td>\n      <td>3.688800</td>\n    </tr>\n    <tr>\n      <td>4920</td>\n      <td>3.712800</td>\n    </tr>\n    <tr>\n      <td>4930</td>\n      <td>3.691900</td>\n    </tr>\n    <tr>\n      <td>4940</td>\n      <td>3.692400</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>3.694400</td>\n    </tr>\n    <tr>\n      <td>4960</td>\n      <td>3.697600</td>\n    </tr>\n    <tr>\n      <td>4970</td>\n      <td>3.701500</td>\n    </tr>\n    <tr>\n      <td>4980</td>\n      <td>3.705200</td>\n    </tr>\n    <tr>\n      <td>4990</td>\n      <td>3.705800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>3.702000</td>\n    </tr>\n    <tr>\n      <td>5010</td>\n      <td>3.667300</td>\n    </tr>\n    <tr>\n      <td>5020</td>\n      <td>3.696300</td>\n    </tr>\n    <tr>\n      <td>5030</td>\n      <td>3.677400</td>\n    </tr>\n    <tr>\n      <td>5040</td>\n      <td>3.704000</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>3.683100</td>\n    </tr>\n    <tr>\n      <td>5060</td>\n      <td>3.689300</td>\n    </tr>\n    <tr>\n      <td>5070</td>\n      <td>3.687200</td>\n    </tr>\n    <tr>\n      <td>5080</td>\n      <td>3.700900</td>\n    </tr>\n    <tr>\n      <td>5090</td>\n      <td>3.709500</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>3.686100</td>\n    </tr>\n    <tr>\n      <td>5110</td>\n      <td>3.690700</td>\n    </tr>\n    <tr>\n      <td>5120</td>\n      <td>3.697100</td>\n    </tr>\n    <tr>\n      <td>5130</td>\n      <td>3.680400</td>\n    </tr>\n    <tr>\n      <td>5140</td>\n      <td>3.702100</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>3.666500</td>\n    </tr>\n    <tr>\n      <td>5160</td>\n      <td>3.701600</td>\n    </tr>\n    <tr>\n      <td>5170</td>\n      <td>3.694800</td>\n    </tr>\n    <tr>\n      <td>5180</td>\n      <td>3.699000</td>\n    </tr>\n    <tr>\n      <td>5190</td>\n      <td>3.698500</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>3.687300</td>\n    </tr>\n    <tr>\n      <td>5210</td>\n      <td>3.697600</td>\n    </tr>\n    <tr>\n      <td>5220</td>\n      <td>3.715700</td>\n    </tr>\n    <tr>\n      <td>5230</td>\n      <td>3.678100</td>\n    </tr>\n    <tr>\n      <td>5240</td>\n      <td>3.700000</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>3.704400</td>\n    </tr>\n    <tr>\n      <td>5260</td>\n      <td>3.674700</td>\n    </tr>\n    <tr>\n      <td>5270</td>\n      <td>3.691100</td>\n    </tr>\n    <tr>\n      <td>5280</td>\n      <td>3.679500</td>\n    </tr>\n    <tr>\n      <td>5290</td>\n      <td>3.718400</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>3.699000</td>\n    </tr>\n    <tr>\n      <td>5310</td>\n      <td>3.698300</td>\n    </tr>\n    <tr>\n      <td>5320</td>\n      <td>3.695900</td>\n    </tr>\n    <tr>\n      <td>5330</td>\n      <td>3.709400</td>\n    </tr>\n    <tr>\n      <td>5340</td>\n      <td>3.681000</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>3.701900</td>\n    </tr>\n    <tr>\n      <td>5360</td>\n      <td>3.712000</td>\n    </tr>\n    <tr>\n      <td>5370</td>\n      <td>3.692100</td>\n    </tr>\n    <tr>\n      <td>5380</td>\n      <td>3.682300</td>\n    </tr>\n    <tr>\n      <td>5390</td>\n      <td>3.706100</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>3.693600</td>\n    </tr>\n    <tr>\n      <td>5410</td>\n      <td>3.699700</td>\n    </tr>\n    <tr>\n      <td>5420</td>\n      <td>3.696200</td>\n    </tr>\n    <tr>\n      <td>5430</td>\n      <td>3.690000</td>\n    </tr>\n    <tr>\n      <td>5440</td>\n      <td>3.690600</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>3.676400</td>\n    </tr>\n    <tr>\n      <td>5460</td>\n      <td>3.689300</td>\n    </tr>\n    <tr>\n      <td>5470</td>\n      <td>3.683800</td>\n    </tr>\n    <tr>\n      <td>5480</td>\n      <td>3.712600</td>\n    </tr>\n    <tr>\n      <td>5490</td>\n      <td>3.689600</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>3.706500</td>\n    </tr>\n    <tr>\n      <td>5510</td>\n      <td>3.672600</td>\n    </tr>\n    <tr>\n      <td>5520</td>\n      <td>3.688100</td>\n    </tr>\n    <tr>\n      <td>5530</td>\n      <td>3.676700</td>\n    </tr>\n    <tr>\n      <td>5540</td>\n      <td>3.671300</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>3.687000</td>\n    </tr>\n    <tr>\n      <td>5560</td>\n      <td>3.678900</td>\n    </tr>\n    <tr>\n      <td>5570</td>\n      <td>3.711600</td>\n    </tr>\n    <tr>\n      <td>5580</td>\n      <td>3.682500</td>\n    </tr>\n    <tr>\n      <td>5590</td>\n      <td>3.680400</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>3.695400</td>\n    </tr>\n    <tr>\n      <td>5610</td>\n      <td>3.690100</td>\n    </tr>\n    <tr>\n      <td>5620</td>\n      <td>3.691000</td>\n    </tr>\n    <tr>\n      <td>5630</td>\n      <td>3.693700</td>\n    </tr>\n    <tr>\n      <td>5640</td>\n      <td>3.695000</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>3.684900</td>\n    </tr>\n    <tr>\n      <td>5660</td>\n      <td>3.685200</td>\n    </tr>\n    <tr>\n      <td>5670</td>\n      <td>3.673600</td>\n    </tr>\n    <tr>\n      <td>5680</td>\n      <td>3.699900</td>\n    </tr>\n    <tr>\n      <td>5690</td>\n      <td>3.698200</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>3.684600</td>\n    </tr>\n    <tr>\n      <td>5710</td>\n      <td>3.711500</td>\n    </tr>\n    <tr>\n      <td>5720</td>\n      <td>3.675500</td>\n    </tr>\n    <tr>\n      <td>5730</td>\n      <td>3.692300</td>\n    </tr>\n    <tr>\n      <td>5740</td>\n      <td>3.696400</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>3.696700</td>\n    </tr>\n    <tr>\n      <td>5760</td>\n      <td>3.676900</td>\n    </tr>\n    <tr>\n      <td>5770</td>\n      <td>3.679200</td>\n    </tr>\n    <tr>\n      <td>5780</td>\n      <td>3.700800</td>\n    </tr>\n    <tr>\n      <td>5790</td>\n      <td>3.703100</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>3.689800</td>\n    </tr>\n    <tr>\n      <td>5810</td>\n      <td>3.695200</td>\n    </tr>\n    <tr>\n      <td>5820</td>\n      <td>3.711900</td>\n    </tr>\n    <tr>\n      <td>5830</td>\n      <td>3.700600</td>\n    </tr>\n    <tr>\n      <td>5840</td>\n      <td>3.690500</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>3.686600</td>\n    </tr>\n    <tr>\n      <td>5860</td>\n      <td>3.673100</td>\n    </tr>\n    <tr>\n      <td>5870</td>\n      <td>3.685600</td>\n    </tr>\n    <tr>\n      <td>5880</td>\n      <td>3.690900</td>\n    </tr>\n    <tr>\n      <td>5890</td>\n      <td>3.677800</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>3.693500</td>\n    </tr>\n    <tr>\n      <td>5910</td>\n      <td>3.695400</td>\n    </tr>\n    <tr>\n      <td>5920</td>\n      <td>3.693400</td>\n    </tr>\n    <tr>\n      <td>5930</td>\n      <td>3.704300</td>\n    </tr>\n    <tr>\n      <td>5940</td>\n      <td>3.694700</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>3.680600</td>\n    </tr>\n    <tr>\n      <td>5960</td>\n      <td>3.686900</td>\n    </tr>\n    <tr>\n      <td>5970</td>\n      <td>3.682500</td>\n    </tr>\n    <tr>\n      <td>5980</td>\n      <td>3.696200</td>\n    </tr>\n    <tr>\n      <td>5990</td>\n      <td>3.689100</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>3.682900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ XLNet training complete!\n✅ Model, tokenizer, and label encoder saved.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"pip install bert_score rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:11:37.564210Z","iopub.execute_input":"2025-05-05T20:11:37.564479Z","iopub.status.idle":"2025-05-05T20:11:40.942650Z","shell.execute_reply.started":"2025-05-05T20:11:37.564449Z","shell.execute_reply":"2025-05-05T20:11:40.941426Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert_score) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import XLNetTokenizer, XLNetForSequenceClassification\nfrom sklearn.metrics import classification_report\nfrom bert_score import score as bert_score\nimport torch\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nfrom rouge_score import rouge_scorer\n\n# Load model and tokenizer\nmodel_path = './xlnet-text-classifier'\ntokenizer = XLNetTokenizer.from_pretrained(model_path)\nmodel = XLNetForSequenceClassification.from_pretrained(model_path)\nmodel.eval()\n\n# Load label encoder\nwith open('./xlnet_label_encoder.pkl', 'rb') as f:\n    le = pickle.load(f)\n\n# Load validation dataset (already tokenized)\nval_texts = val_dataset['input_ids']\nval_attention = val_dataset['attention_mask']\nval_labels = val_dataset['label']\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Inference\npreds = []\nreferences = []\n\nwith torch.no_grad():\n    for i in tqdm(range(0, len(val_texts), 8), desc=\"Evaluating...\"):  # batch size = 8\n        input_ids = val_texts[i:i+8].to(device)\n        attention_mask = val_attention[i:i+8].to(device)\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        batch_preds = torch.argmax(logits, dim=-1).cpu().tolist()\n        preds.extend(batch_preds)\n        references.extend(val_labels[i:i+8].tolist())\n\n# Decode predicted and reference labels to strings\npred_labels = le.inverse_transform(preds)\ntrue_labels = le.inverse_transform(references)\n\n# Convert to text for BERTScore\npred_texts = [str(label) for label in pred_labels]\nref_texts = [str(label) for label in true_labels]\n\n# BERTScore\nP, R, F1 = bert_score(pred_texts, ref_texts, lang=\"en\", model_type=\"bert-base-uncased\")\n\nprint(f\"\\n📊 BERTScore Evaluation:\")\nprint(f\"Precision: {P.mean().item():.4f}\")\nprint(f\"Recall:    {R.mean().item():.4f}\")\nprint(f\"F1 Score:  {F1.mean().item():.4f}\")\n\n# ROUGE Score\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\nrouge1_scores = []\nrouge2_scores = []\nrougeL_scores = []\n\nfor pred, ref in zip(pred_labels, true_labels):\n    scores = scorer.score(ref, pred)\n    rouge1_scores.append(scores['rouge1'].fmeasure)\n    rouge2_scores.append(scores['rouge2'].fmeasure)\n    rougeL_scores.append(scores['rougeL'].fmeasure)\n\navg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\navg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\navg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n\nprint(\"\\n📊 ROUGE Score Evaluation:\")\nprint(f\"ROUGE-1: {avg_rouge1:.4f}\")\nprint(f\"ROUGE-2: {avg_rouge2:.4f}\")\nprint(f\"ROUGE-L: {avg_rougeL:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T20:11:47.647866Z","iopub.execute_input":"2025-05-05T20:11:47.648713Z","iopub.status.idle":"2025-05-05T20:33:39.316430Z","shell.execute_reply.started":"2025-05-05T20:11:47.648680Z","shell.execute_reply":"2025-05-05T20:33:39.315792Z"}},"outputs":[{"name":"stderr","text":"Evaluating...: 100%|██████████| 1500/1500 [21:37<00:00,  1.16it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29cd6f8ccac14f97acab161370c4b90a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d20544b157c4f919dfcad3368b99cfa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e59a2f67c442f09baa48980195a38f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fa23d3831a3462997c656a1785e194f"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"512d2fd2e0f149dbbecdb29f93e71a74"}},"metadata":{}},{"name":"stdout","text":"\n📊 BERTScore Evaluation:\nPrecision: 0.5605\nRecall:    0.5069\nF1 Score:  0.5278\n\n📊 ROUGE Score Evaluation:\nROUGE-1: 0.0250\nROUGE-2: 0.0250\nROUGE-L: 0.0250\n","output_type":"stream"}],"execution_count":9}]}